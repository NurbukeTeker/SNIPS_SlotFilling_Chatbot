
# -*- coding: utf-8 -*-
"""BERT-for-Joint-Intent-detection-and-slot-filling_ATIS.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1rMTwZ9iZOHaLWPb-RYa5fWBNIVWOR6HI
"""


import tensorflow as tf
import os
from tqdm import tqdm
import pandas as pd
import numpy as np
from sklearn.preprocessing import LabelEncoder
from transformers import BertTokenizer, TFBertModel
import warnings
warnings.filterwarnings('ignore')
from tensorflow.keras.layers import Dropout, Dense
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.losses import SparseCategoricalCrossentropy
from tensorflow.keras.metrics import SparseCategoricalAccuracy
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, LearningRateScheduler, ModelCheckpoint, TensorBoard
from seqeval.metrics import classification_report
import shutil
import pickle
from seqeval.metrics import classification_report

SNIP_DATA_PATH = 'atis'
BERT_MODEL = 'bert-base-uncased'
MAX_SEQ_LEN = 50

def model_func():
    def get_data(data_path):
      data_list = []
      with open(data_path, 'r', encoding='utf8') as fp:
        for line in tqdm(fp.readlines()):
          data_list.append(line.strip())
      return data_list
    
    
    train_seq_in = get_data(os.path.join(os.path.join(SNIP_DATA_PATH,'train'),'seq.in'))
    train_seq_out = get_data(os.path.join(os.path.join(SNIP_DATA_PATH,'train'),'seq.out'))
    train_labels = get_data(os.path.join(os.path.join(SNIP_DATA_PATH,'train'),'label'))
    
    val_seq_in = get_data(os.path.join(os.path.join(SNIP_DATA_PATH,'valid'),'seq.in'))
    val_seq_out = get_data(os.path.join(os.path.join(SNIP_DATA_PATH,'valid'),'seq.out'))
    val_labels = get_data(os.path.join(os.path.join(SNIP_DATA_PATH,'valid'),'label'))
    
    test_seq_in = get_data(os.path.join(os.path.join(SNIP_DATA_PATH,'test'),'seq.in'))
    test_seq_out = get_data(os.path.join(os.path.join(SNIP_DATA_PATH,'test'),'seq.out'))
    test_labels = get_data(os.path.join(os.path.join(SNIP_DATA_PATH,'test'),'label'))
    
    columns_name = ['seq_in','seq_out','label']
    df_train = pd.DataFrame(list(zip(train_seq_in,train_seq_out,train_labels)), columns=columns_name)
    df_val = pd.DataFrame(list(zip(val_seq_in,val_seq_out,val_labels)), columns=columns_name)
    df_test = pd.DataFrame(list(zip(test_seq_in,test_seq_out,test_labels)),columns=columns_name)
    
    len(df_test)
    
    print(df_train.shape)
    print("\n")
    df_train.head()
    
    df_train.isna().any()
    
    df_val.isna().any()
    
    df_test.isna().any()
    
    df_train["label"].value_counts()
    
    total_intent_num = len(df_train['label'].unique())
    total_intent_num
    
    """enocde intents"""
    
    le = LabelEncoder()
    df_all["labels"] = df_train['label'] + df_val['label'] + df_test['label']
    le.fit(df_train['label'])
    
    train_labels_encoded = le.transform(df_train['label'].values)
    val_labels_encoded = le.transform(df_val['label'].values)
    test_labels_encoded = le.transform(df_test['label'].values)
    
    """BERT"""
    
    tokenizer = BertTokenizer.from_pretrained(BERT_MODEL)
    
    tokenizer.vocab_size
    
    bert_vocab_items = list(tokenizer.vocab.items())
    print(bert_vocab_items[:10])
    print("\n")
    print(bert_vocab_items[1000:1100])
    
    def encode_dataset(tokenizer, text_sequences, max_length):
        token_ids = np.zeros(shape=(len(text_sequences), max_length),
                              dtype=np.int32)
        for i, text_sequence in enumerate(text_sequences):
            encoded = tokenizer.encode(text_sequence)
            token_ids[i, 0:len(encoded)] = encoded
        attention_masks = (token_ids != 0).astype(np.int32)
        return {"input_ids": token_ids, "attention_masks": attention_masks}
    
    
    train_bert_input = encode_dataset(tokenizer, df_train["seq_in"], MAX_SEQ_LEN)
    train_bert_input["input_ids"]
    
    train_bert_input["attention_masks"]
    
    valid_bert_input = encode_dataset(tokenizer, df_val["seq_in"], MAX_SEQ_LEN)
    test_bert_input = encode_dataset(tokenizer, df_test["seq_in"], MAX_SEQ_LEN)
    
    
    """Bert sample modeling"""
    
    tf.keras.backend.clear_session()
    tf_bert_model = TFBertModel.from_pretrained(BERT_MODEL)
    tf_bert_model.summary()
    
    encoded_valid = encode_dataset(tokenizer, df_val["seq_in"], MAX_SEQ_LEN)
    encoded_valid["input_ids"]
    
    
    
    bert_output = tf_bert_model(encoded_valid)
    
    print("Type: ",type(bert_output))
    print("length: ", len(bert_output))
    print("Shape of first item: ", bert_output[0].shape)
    print("Shape of second item: ", bert_output[1].shape)
    print(type(bert_output[0]), type(bert_output[1]))
    
    """slot filling"""
    
    #removed -,_ from the filters, so not to break labels like B-object_name
    seq_out_tokenizer = Tokenizer(filters='!"#$%&()*+,./:;<=>?@[\\]^`{|}~\t\n', oov_token="UNK",lower=False)
    seq_out_tokenizer.fit_on_texts(df_train["seq_out"].tolist())
    
    seq_out_word_to_index = seq_out_tokenizer.word_index
    len(seq_out_word_to_index)
    
    seq_out_word_to_index
    
    def encode_tokens(text_seq, label_seq, word_to_index, tokenizer):
    
      encoded_seq_labels = []
      extra_keys = 0
      extra_key_names = []
      
      for i, (text, labels) in enumerate(zip(text_seq, label_seq)):
        sent_level_label_encoding=[]
        for word, word_label in zip(text.split(), labels.split()):
          word_level_label_encoding = []
          word_tokens = tokenizer.tokenize(word)
          for w in word_tokens:
            #handling when word has word level tokenizatation, and label start with B-
            if w.startswith("#") and word_label.startswith("B-"):
              word_label = word_label.replace("B-","I-")
              if word_label not in word_to_index.keys():
                extra_keys +=1
                extra_key_names.append(word_label)
                word_to_index[word_label] = len(word_to_index)
            word_level_label_encoding.append(word_label) 
          sent_level_label_encoding.extend(word_level_label_encoding)
        #assert to check weather we have same number of token and labels
        assert(len(sent_level_label_encoding) == len(tokenizer.tokenize(text)))
        encoded_seq_labels.append(" ".join(sent_level_label_encoding))
    
      print("Total number of keys added\n",extra_keys)
      print("Extra keys are,\n ", extra_key_names)
      return encoded_seq_labels
    
    train_slots_encoded = encode_tokens(df_train['seq_in'].tolist(), df_train['seq_out'].tolist(), seq_out_word_to_index, tokenizer)
    val_slots_encoded = encode_tokens(df_val['seq_in'].tolist(), df_val['seq_out'].tolist(), seq_out_word_to_index, tokenizer)
    test_slots_encoded = encode_tokens(df_test['seq_in'].tolist(), df_test['seq_out'].tolist(), seq_out_word_to_index, tokenizer)
    
    len(train_slots_encoded), len(val_slots_encoded), len(test_slots_encoded)
    
    seq_out_tokenizer = Tokenizer(oov_token="UNK",lower=False,filters='!"#$%&()*+,./:;<=>?@\\^`{|}~\t\n')
    seq_out_tokenizer.fit_on_texts(train_slots_encoded)
    
    len(seq_out_tokenizer.word_index)
    
    seq_out_tokenizer.word_index
    
    seq_out_tokenizer.word_index['PAD'] = 0
    seq_out_tokenizer.index_word[0] = 'PAD'
    
    len(seq_out_tokenizer.word_index), len(seq_out_tokenizer.index_word)
    
    train_slots_tokenized = seq_out_tokenizer.texts_to_sequences(train_slots_encoded)
    val_slots_tokenized = seq_out_tokenizer.texts_to_sequences(val_slots_encoded)
    test_slots_tokenized = seq_out_tokenizer.texts_to_sequences(test_slots_encoded)
    
    len(train_slots_tokenized), len(val_slots_tokenized), len(test_slots_tokenized)
    
    def prepare_slots_for_bert(tokenized_slots, max_len):
    
      final = np.zeros(shape=(len(tokenized_slots),max_len), dtype='int32')
    
      for i,slot in enumerate(tokenized_slots):
        final[i, 1:len(slot)+1] = slot
      return final
    
    train_slots = prepare_slots_for_bert(train_slots_tokenized, MAX_SEQ_LEN)
    valid_slots = prepare_slots_for_bert(val_slots_tokenized, MAX_SEQ_LEN)
    test_slots = prepare_slots_for_bert(test_slots_tokenized, MAX_SEQ_LEN)
    
    train_slots_encoded
    
    train_slots.shape, valid_slots.shape, test_slots.shape
    
    def get_slots_with_pad(text_seq, maxlen):
      final = np.full((len(text_seq), maxlen), "PAD", dtype='U32')
    
      for i,text in tqdm(enumerate(text_seq)):
        splitted_text = text.split()
        final[i, 1:len(splitted_text)+1] = splitted_text
      return final
    
    train_slots_encoed_with_pad_token = get_slots_with_pad(train_slots_encoded, MAX_SEQ_LEN)
    val_slots_encoded_with_pad_token =  get_slots_with_pad(val_slots_encoded, MAX_SEQ_LEN)
    test_slots_encoded_with_pad_token = get_slots_with_pad(test_slots_encoded, MAX_SEQ_LEN)
    
    train_slots_encoed_with_pad_token
    
    len(train_slots_encoed_with_pad_token), len(val_slots_encoded_with_pad_token), len(test_slots_encoded_with_pad_token)
    
    
    """Model"""
    
    class JointIntentAndSlotFillingModel(tf.keras.Model):
    
        def __init__(self, total_intent_no=None, total_slot_no=None,
                      model_name=BERT_MODEL, dropout_prob=0.1):
            super().__init__()
            self.bert = TFBertModel.from_pretrained(model_name)
            self.dropout = Dropout(dropout_prob)
            self.intent_classifier = Dense(total_intent_no, activation='softmax')
            self.slot_classifier = Dense(total_slot_no, activation='softmax')
    
        def call(self, inputs, **kwargs):
            bert_output = self.bert(inputs)
    
            sequence_output = self.dropout(bert_output[0])
            slots_predicted = self.slot_classifier(sequence_output)
    
            pooled_output = self.dropout(bert_output[1])
            intent_predicted = self.intent_classifier(pooled_output)
    
            return slots_predicted, intent_predicted
    
    joint_model = JointIntentAndSlotFillingModel(
        total_intent_no=7, total_slot_no=77,dropout_prob=0.1)
    
    """create callback"""
    
    model_path = '/content/drive/MyDrive/SlotFilling/atis/joint_model'
    model_name = "joint_model_weights_{val_loss:.2f}.ckpt"
    if os.path.exists(model_path):
      print("Model path exist, clearing all files under model_path")
      shutil.rmtree(model_path)
    else:
      print("Creating model_path")
      os.makedirs(model_path)
    
    model_chk_point = ModelCheckpoint(filepath=os.path.join(model_path,model_name),monitor="val_loss",save_best_only=True,save_weights_only=True)
    
    early_stopping = EarlyStopping(monitor="val_loss",min_delta=0.0001,patience=4,verbose=1)
    
    #tensorboard callback
    tensorboard_path = "/content/drive/MyDrive/SlotFilling/atis/Tensorboard/logs/"
    if os.path.exists(tensorboard_path):
      print("Tensorboard path exists, clearing all files under tensorboard_path")
      shutil.rmtree(tensorboard_path)
    else:
      print("Creating tensorboard path")
      os.makedirs(tensorboard_path)
    
    tensorboard_cb = TensorBoard(log_dir=tensorboard_path)
    
    #prepare callback list
    callback_list = [model_chk_point, tensorboard_cb, early_stopping]
    
    """compile model"""
    
    opt = Adam(learning_rate=3e-5, epsilon=1e-08)
    losses = [SparseCategoricalCrossentropy(from_logits=False),
              SparseCategoricalCrossentropy(from_logits=False)]
    metrics = [SparseCategoricalAccuracy('accuracy')]
    
    joint_model.compile(optimizer=opt, loss=losses, metrics=metrics)
    
    """fit model"""
    
    history = joint_model.fit(
        train_bert_input, (train_slots,train_labels_encoded),
        validation_data=(valid_bert_input, (valid_slots, val_labels_encoded)),
        epochs=10, batch_size=128,callbacks=callback_list)
    
    joint_model.summary()
    
    joint_model.save("/content/drive/MyDrive/SlotFilling/atis/slotfilling_model_atis_new2/model.tf")
    
    index_to_word =seq_out_tokenizer.index_word
    
    
    tokenizer = BertTokenizer.from_pretrained(BERT_MODEL)
    return tokenizer, joint_model, le, index_to_word




def show_predictions(text):

  tokenized_sent = tokenizer.encode(text)

  predicted_slots, predicted_intents = joint_model.predict([tokenized_sent])

  intent = le.inverse_transform([np.argmax(predicted_intents)])
  print("="*5, "INTENT", "="*5)
  print(intent)

  slots = np.argmax(predicted_slots, axis=-1)

  slots = [index_to_word[w_idx] for w_idx in slots[0]]

  print("\n")
  print("="*5, "SLOTS","="*5)
  for w,l in zip(tokenizer.tokenize(text),slots[1:-1]):
    print(w,  l)
    
  return intent, tokenizer.tokenize(text),slots[1:-1]


# show_predictions("Is there rain in India")

# show_predictions("Mugulunage add this to my playlist")

# show_predictions('Book a table for two at Le Ritz for Friday night!')

# show_predictions('I would like to listen to Anima by Thom Yorke.')

# show_predictions('i rate hero movie as a 4 out of 5')


text = "Can you please rate Hero movie in scale of 1 to 5"
#a,b ,c=  show_predictions("Can you please rate Hero movie in scale of 1 to 5")